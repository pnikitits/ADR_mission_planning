{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from ADR_Environment import ADR_Environment\n",
    "from rl_glue import RLGlue\n",
    "from Complete_pytorch import Transition, ReplayBuffer, ActionValueNetwork\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\" \n",
    "    Deep Q-Learning Agent with Experience Replay \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = 'dqn'\n",
    "\n",
    "    def agent_init(self, agent_config):\n",
    "        self.device = agent_config['device']\n",
    "        self.replay_buffer = ReplayBuffer(agent_config[\"replay_buffer_size\"],\n",
    "                                          agent_config[\"minibatch_size\"]\n",
    "                                          )\n",
    "        self.policy_network = ActionValueNetwork(agent_config[\"network_config\"]).to(self.device)\n",
    "        self.target_network = ActionValueNetwork(agent_config[\"network_config\"]).to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_network.parameters(),\n",
    "                              lr = agent_config[\"optimizer_config\"][\"step_size\"],\n",
    "                              betas = (agent_config[\"optimizer_config\"][\"beta_m\"], agent_config[\"optimizer_config\"][\"beta_v\"]),\n",
    "                              eps = agent_config[\"optimizer_config\"][\"epsilon\"],\n",
    "                              amsgrad = True)\n",
    "        self.num_actions = agent_config[\"network_config\"][\"num_actions\"]\n",
    "        self.num_replay = agent_config[\"num_replay_updates_per_step\"]\n",
    "        self.discount = agent_config[\"gamma\"]\n",
    "        self.tau = agent_config[\"tau\"]\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.ep_loss = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - state (Numpy array): the state.\n",
    "        Returns:\n",
    "            - the action (int).\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state, device = self.device, dtype = torch.float32)\n",
    "        action = self.policy_network.select_action(state, self.tau)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"\n",
    "        The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            - state (Numpy array): the state from the\n",
    "                environment's env_start function.\n",
    "        Returns:\n",
    "            - The first action the agent takes (int).\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"State in agent_start =\" , state[1])\n",
    "\n",
    "        self.ep_loss = 0\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state[1]])\n",
    "\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"\n",
    "        A step taken by the agent.\n",
    "        Args:\n",
    "            - reward (float): the reward received for taking the last action taken\n",
    "            - state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            - The action the agent is taking (int).\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        state = np.array([state])\n",
    "        #print('***' * 50)\n",
    "        #print(\"state  = \",state)\n",
    "        #print('***' * 50)\n",
    "\n",
    "        action = self.policy(state)\n",
    "        self.replay_buffer.append(self.last_state , self.last_action , reward , 0 , state)\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.optimize_network(experiences)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"\n",
    "        Run when the agent terminates.\n",
    "        Args:\n",
    "            - reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        state = np.zeros_like(self.last_state)\n",
    "        self.replay_buffer.append(self.last_state , self.last_action , reward , 1 , state)\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.optimize_network(experiences) \n",
    "\n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        elif message == \"get_loss\":\n",
    "            return self.ep_loss\n",
    "        else:\n",
    "            raise Exception(\"Unrecognised Message !\")\n",
    "\n",
    "    def optimize_network(self, experiences):\n",
    "        \"\"\"\n",
    "        Optimize the policy network using the experiences\n",
    "        \"\"\"        \n",
    "        # transpose the batch from batch-array of transitions to Transition of batch-array\n",
    "        batch = Transition(*zip(*experiences))\n",
    "\n",
    "        # transform all batches into tensors\n",
    "        non_final_mask = torch.tensor([not s for s in batch.terminal], device=self.device, dtype=torch.int64)\n",
    "        non_final_next_states = torch.tensor(batch.next_state, device=self.device, dtype=torch.float32).squeeze(1)\n",
    "        state_batch = torch.tensor(batch.state, device=self.device, dtype=torch.float32).squeeze(1)\n",
    "        action_batch = torch.tensor(batch.action, device=self.device).unsqueeze(-1)\n",
    "        reward_batch = torch.tensor(batch.reward, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q(s_t, a)\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros(self.replay_buffer.minibatch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "\n",
    "        # Compute the expected Q values (TD targets)\n",
    "        expected_state_action_values = (next_state_values * self.discount) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.retain_grad()\n",
    "        loss.backward()\n",
    "        self.ep_loss += loss.to(torch.int32)\n",
    "\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100) \n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(environment , agent , environment_parameters , agent_parameters , experiment_parameters):\n",
    "    \"\"\"\n",
    "    Run the experiment\n",
    "    \"\"\"\n",
    "\n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"],\n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    env_info = {}\n",
    "    agent_info = agent_parameters\n",
    "    for run in range(1 , experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = 0 #run\n",
    "        agent_info[\"network_config\"][\"seed\"] = 0 #run\n",
    "        env_info[\"seed\"] = 0 #run\n",
    "        rl_glue.rl_init(agent_info , env_info)\n",
    "\n",
    "        seed = agent_info[\"seed\"]\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        gpu_use = experiment_parameters['gpu_use']\n",
    "        if gpu_use and torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        ep_count = 0\n",
    "        for episode in range(1 , experiment_parameters[\"num_episodes\"]+1):\n",
    "            ep_count += 1\n",
    "            #environment.pass_count(environment, message=f\"Ep : {ep_count}\")\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "\n",
    "            # Get data from episode\n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            ep_loss = rl_glue.rl_agent_message((\"get_loss\"))\n",
    "            fuel_limit, time_limit, impossible_dt, impossible_binary_flag = rl_glue.environment.get_term_reason()\n",
    "            avg_fuel_used = rl_glue.environment.get_fuel_use_average()\n",
    "            avg_time_used = rl_glue.environment.get_time_use_average()\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"episode loss\": ep_loss,\n",
    "                    \"episode reward\": episode_reward ,\n",
    "                    \"fuel limit\": fuel_limit,\n",
    "                    \"time limit\": time_limit,\n",
    "                    \"impossible_dt\": impossible_dt,\n",
    "                    \"impossible_binary_flag\": impossible_binary_flag,\n",
    "                    \"average fuel used\":avg_fuel_used,\n",
    "                    \"average time used\":avg_time_used\n",
    "                })\n",
    "\n",
    "\n",
    "    wandb.log({\"avg_reward\": sum(agent_sum_reward[0])/experiment_parameters[\"num_episodes\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    a = wandb.init()\n",
    "    weights_file = None #'models/test_weights.pth'\n",
    "    experiment_parameters = {\"num_runs\":1,\n",
    "                                \"num_episodes\":2000,\n",
    "                                \"timeout\":2000,\n",
    "                                \"gpu_use\":True,\n",
    "                                \"track_wandb\":False}\n",
    "    environment_parameters = {}\n",
    "    current_env = ADR_Environment\n",
    "    agent_parameters = {\"network_config\":{\"state_dim\":25,\n",
    "                                            \"num_hidden_units\":512,\n",
    "                                            \"num_actions\":300,\n",
    "                                            \"weights_file\":weights_file},\n",
    "                            \"optimizer_config\":{\"step_size\": wandb.config.learning_rate, # working value 1e-3\n",
    "                                                \"beta_m\":0.9,\n",
    "                                                \"beta_v\":0.999,\n",
    "                                                \"epsilon\":1e-8},\n",
    "                            \"replay_buffer_size\":wandb.config.replay_buffer_size,\n",
    "                            \"minibatch_size\":wandb.config.minibatch_size,\n",
    "                            \"num_replay_updates_per_step\": wandb.config.replay_updates_per_step,\n",
    "                            \"gamma\":wandb.config.gamma,\n",
    "                            \"tau\":wandb.config.tau,\n",
    "                            \"seed\":0\n",
    "                            }\n",
    "\n",
    "    # Set device\n",
    "    gpu_use = experiment_parameters['gpu_use']\n",
    "    \n",
    "    if gpu_use and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    agent_parameters['device'] = device\n",
    "    print(device)\n",
    "    current_agent = Agent\n",
    "\n",
    "    run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sweep_config/grid.yaml\") as file: # change file name to use different sweep\n",
    "    sweep_configuration = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"HPO-ADR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=main) # count = 20 if bayesian search, nothing if grid search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
