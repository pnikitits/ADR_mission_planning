{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from ADR_Environment import ADR_Environment\n",
    "from rl_glue import RLGlue\n",
    "from Complete_pytorch import Transition, ReplayBuffer, ActionValueNetwork\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\" \n",
    "    Deep Q-Learning Agent with Experience Replay \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = 'dqn'\n",
    "\n",
    "    def agent_init(self, agent_config):\n",
    "        self.device = agent_config['device']\n",
    "        self.replay_buffer = ReplayBuffer(agent_config[\"replay_buffer_size\"],\n",
    "                                          agent_config[\"minibatch_size\"]\n",
    "                                          )\n",
    "        self.policy_network = ActionValueNetwork(agent_config[\"network_config\"])\n",
    "        self.target_network = ActionValueNetwork(agent_config[\"network_config\"])\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_network.parameters(),\n",
    "                              lr = agent_config[\"optimizer_config\"][\"step_size\"],\n",
    "                              betas = (agent_config[\"optimizer_config\"][\"beta_m\"], agent_config[\"optimizer_config\"][\"beta_v\"]),\n",
    "                              eps = agent_config[\"optimizer_config\"][\"epsilon\"],\n",
    "                              amsgrad = True)\n",
    "        self.num_actions = agent_config[\"network_config\"][\"num_actions\"]\n",
    "        self.num_replay = agent_config[\"num_replay_updates_per_step\"]\n",
    "        self.discount = agent_config[\"gamma\"]\n",
    "        self.tau = agent_config[\"tau\"]\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.ep_loss = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - state (Numpy array): the state.\n",
    "        Returns:\n",
    "            - the action (int).\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state, device = self.device, dtype = torch.float32)\n",
    "        action = self.policy_network.select_action(state, self.tau)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"\n",
    "        The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            - state (Numpy array): the state from the\n",
    "                environment's env_start function.\n",
    "        Returns:\n",
    "            - The first action the agent takes (int).\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"State in agent_start =\" , state[1])\n",
    "\n",
    "        self.ep_loss = 0\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state[1]])\n",
    "\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"\n",
    "        A step taken by the agent.\n",
    "        Args:\n",
    "            - reward (float): the reward received for taking the last action taken\n",
    "            - state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            - The action the agent is taking (int).\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        state = np.array([state])\n",
    "        #print('***' * 50)\n",
    "        #print(\"state  = \",state)\n",
    "        #print('***' * 50)\n",
    "\n",
    "        action = self.policy(state)\n",
    "        self.replay_buffer.append(self.last_state , self.last_action , reward , 0 , state)\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.optimize_network(experiences)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"\n",
    "        Run when the agent terminates.\n",
    "        Args:\n",
    "            - reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        state = np.zeros_like(self.last_state)\n",
    "        self.replay_buffer.append(self.last_state , self.last_action , reward , 1 , state)\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.optimize_network(experiences) \n",
    "\n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        elif message == \"get_loss\":\n",
    "            return self.ep_loss\n",
    "        else:\n",
    "            raise Exception(\"Unrecognised Message !\")\n",
    "\n",
    "    def optimize_network(self, experiences):\n",
    "        \"\"\"\n",
    "        Optimize the policy network using the experiences\n",
    "        \"\"\"        \n",
    "        # transpose the batch from batch-array of transitions to Transition of batch-array\n",
    "        batch = Transition(*zip(*experiences))\n",
    "\n",
    "        # transform all batches into tensors\n",
    "        non_final_mask = torch.tensor([not s for s in batch.terminal], device=self.device, dtype=torch.int64)\n",
    "        non_final_next_states = torch.tensor(batch.next_state, device=self.device, dtype=torch.float32).squeeze(1)\n",
    "        state_batch = torch.tensor(batch.state, device=self.device, dtype=torch.float32).squeeze(1)\n",
    "        action_batch = torch.tensor(batch.action, device=self.device).unsqueeze(-1)\n",
    "        reward_batch = torch.tensor(batch.reward, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q(s_t, a)\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros(self.replay_buffer.minibatch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "\n",
    "        # Compute the expected Q values (TD targets)\n",
    "        expected_state_action_values = (next_state_values * self.discount) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.retain_grad()\n",
    "        loss.backward()\n",
    "        self.ep_loss += loss.to(torch.int32)\n",
    "\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100) \n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(environment , agent , environment_parameters , agent_parameters , experiment_parameters):\n",
    "    \"\"\"\n",
    "    Run the experiment\n",
    "    \"\"\"\n",
    "\n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"],\n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    env_info = {}\n",
    "    agent_info = agent_parameters\n",
    "    for run in range(1 , experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = 0 #run\n",
    "        agent_info[\"network_config\"][\"seed\"] = 0 #run\n",
    "        env_info[\"seed\"] = 0 #run\n",
    "        rl_glue.rl_init(agent_info , env_info)\n",
    "\n",
    "        seed = agent_info[\"seed\"]\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        gpu_use = experiment_parameters['gpu_use']\n",
    "        if gpu_use and torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        ep_count = 0\n",
    "        for episode in range(1 , experiment_parameters[\"num_episodes\"]+1):\n",
    "            ep_count += 1\n",
    "            #environment.pass_count(environment, message=f\"Ep : {ep_count}\")\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "\n",
    "            # Get data from episode\n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            ep_loss = rl_glue.rl_agent_message((\"get_loss\"))\n",
    "            fuel_limit, time_limit, impossible_dt, impossible_binary_flag = rl_glue.environment.get_term_reason()\n",
    "            avg_fuel_used = rl_glue.environment.get_fuel_use_average()\n",
    "            avg_time_used = rl_glue.environment.get_time_use_average()\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"episode loss\": ep_loss,\n",
    "                    \"episode reward\": episode_reward ,\n",
    "                    \"fuel limit\": fuel_limit,\n",
    "                    \"time limit\": time_limit,\n",
    "                    \"impossible_dt\": impossible_dt,\n",
    "                    \"impossible_binary_flag\": impossible_binary_flag,\n",
    "                    \"average fuel used\":avg_fuel_used,\n",
    "                    \"average time used\":avg_time_used\n",
    "                })\n",
    "\n",
    "\n",
    "    wandb.log({\"avg_reward\": sum(agent_sum_reward[0])/experiment_parameters[\"num_episodes\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    a = wandb.init()\n",
    "    weights_file = None #'models/test_weights.pth'\n",
    "    experiment_parameters = {\"num_runs\":1,\n",
    "                                \"num_episodes\":2000,\n",
    "                                \"timeout\":2000,\n",
    "                                \"gpu_use\":False,\n",
    "                                \"track_wandb\":False}\n",
    "    environment_parameters = {}\n",
    "    current_env = ADR_Environment\n",
    "    agent_parameters = {\"network_config\":{\"state_dim\":25,\n",
    "                                            \"num_hidden_units\":512,\n",
    "                                            \"num_actions\":300,\n",
    "                                            \"weights_file\":weights_file},\n",
    "                            \"optimizer_config\":{\"step_size\": wandb.config.learning_rate, # working value 1e-3\n",
    "                                                \"beta_m\":0.9,\n",
    "                                                \"beta_v\":0.999,\n",
    "                                                \"epsilon\":1e-8},\n",
    "                            \"replay_buffer_size\":wandb.config.replay_buffer_size,\n",
    "                            \"minibatch_size\":wandb.config.minibatch_size,\n",
    "                            \"num_replay_updates_per_step\": wandb.config.replay_updates_per_step,\n",
    "                            \"gamma\":wandb.config.gamma,\n",
    "                            \"tau\":wandb.config.tau,\n",
    "                            \"seed\":0\n",
    "                            }\n",
    "\n",
    "    # Set device\n",
    "    gpu_use = experiment_parameters['gpu_use']\n",
    "    if gpu_use and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using the GPU\")\n",
    "    elif gpu_use and not torch.cuda.is_available():\n",
    "        device = torch.device(\"cpu\")\n",
    "        print('GPU is not available. Using CPU')\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        agent_parameters['device'] = device\n",
    "        print(\"Using the CPU\")\n",
    "\n",
    "    current_agent = Agent\n",
    "    \n",
    "    run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sweep_config/grid.yaml\") as file: # change file name to use different sweep\n",
    "    sweep_configuration = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: j39xqw5x\n",
      "Sweep URL: https://wandb.ai/pnikitits_1/HPO-ADR/sweeps/j39xqw5x\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"HPO-ADR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y3xack4r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tminibatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 250000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_updates_per_step: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttau: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpnikitits\u001b[0m (\u001b[33mpnikitits_1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\pniki\\Documents\\GitHub\\ADR_mission_planning\\wandb\\run-20240318_181428-y3xack4r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pnikitits_1/HPO-ADR/runs/y3xack4r' target=\"_blank\">olive-sweep-1</a></strong> to <a href='https://wandb.ai/pnikitits_1/HPO-ADR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pnikitits_1/HPO-ADR/sweeps/j39xqw5x' target=\"_blank\">https://wandb.ai/pnikitits_1/HPO-ADR/sweeps/j39xqw5x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pnikitits_1/HPO-ADR' target=\"_blank\">https://wandb.ai/pnikitits_1/HPO-ADR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pnikitits_1/HPO-ADR/sweeps/j39xqw5x' target=\"_blank\">https://wandb.ai/pnikitits_1/HPO-ADR/sweeps/j39xqw5x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pnikitits_1/HPO-ADR/runs/y3xack4r' target=\"_blank\">https://wandb.ai/pnikitits_1/HPO-ADR/runs/y3xack4r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]c:\\Users\\pniki\\Documents\\GitHub\\ADR_mission_planning\\Simulator\\CustomLowLevel.py:49: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (Array(float64, 1, 'A', False, aligned=True), Array(float64, 1, 'A', False, aligned=True))\u001b[0m\u001b[0m\u001b[0m\n",
      "  r_i = norm(r_i)\n",
      "  0%|          | 1/2000 [00:06<3:22:25,  6.08s/it]C:\\Users\\pniki\\AppData\\Local\\Temp\\ipykernel_2312\\865038775.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:279.)\n",
      "  non_final_next_states = torch.tensor(batch.next_state, device=self.device, dtype=torch.float32).squeeze(1)\n",
      " 18%|█▊        | 366/2000 [00:32<01:24, 19.45it/s]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=main) # count = 20 if bayesian search, nothing if grid search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
